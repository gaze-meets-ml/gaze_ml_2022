{
  

  
  

  
      "page1": {
          "title": "Presentations",
          "content": "We want to congratulate the authors for the following accepted work to be presented at the first-ever Gaze Meets ML workshop on December 3rd, 2022 in conjunction with NeurIPS 2022. We will be updating this page with more detailed schedule for the workshop shortly. . Full Papers . Contrastive Representation Learning for Gaze Estimation (Oral) | Electrode Clustering and Bandpass Analysis of EEG Data for Gaze Estimation (Oral) | Facial Composite Generation with Iterative Human Feedback (Oral) | Intention Estimation via Gaze for Robot Guidance in Hierarchical Tasks (Oral) | Modeling Human Eye Movements with Neural Networks in a Maze-Solving Task (Oral) | SecNet: Semantic Eye Completion in Implicit Field (Oral) | Appearance-Based Gaze Estimation for Driver Monitoring (Poster) | Decoding Attention from Gaze: A Benchmark Dataset and End-to-End Models (Poster) | Federated Learning for Appearance-based Gaze Estimation in the Wild (Poster) | Generating Attention Maps from Eye-gaze for the Diagnosis of Alzheimer’s Disease (Poster) | Integrating eye gaze into machine learning using fractal curves (Poster) | Learning to count visual objects by combining “what” and “where” in recurrent memoryd (Poster) | Selection of XAI Methods Matters: Evaluation of Feature Attribution Methods for Oculomotoric Biometric Identification (Poster) | Skill, or Style? Classification of Fetal Sonography Eye-Tracking Data (Poster) | . Extended Abstracts . Simulating Human Gaze with Neural Visual Attention (Oral) | Comparing radiologists’ gaze and saliency maps generated by interpretability methods for chest x-rays (Poster) | Do They Look Where They Go? Gaze Classification During Walking (Poster) | Time-to-Saccade metrics for real-world evaluation (Poster) | .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/presentations/",
          "relUrl": "/presentations/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Sponsorship Areas . Currently, we are looking for sponsors for the following areas: . Best Paper Prizes: We are looking to attract paper submissions in several different research areas related to integrating human attention and eye gaze in machine learning. The prizes can be in different forms: . Cash (preferable) . | Eye gaze tracking related resources . | GPUs . | . | Student Travel Awards: We are looking to providing support to student participants, especially enhancing our diversity and inclusion efforts. . | Support for the Invited Speakers: We have an amazing lineup of experts at the intersection of machine learning, neuroscience, psychology and eye gaze. We are looking to provide support for conference registration and, if possible, a small honorarium and partial coverage of travel costs. . | . Benefits . The Gaze Meets ML workshop at NeurIPS 2022 comes with several benefits for sponsor partners: . Acknowledgment and banner on our website (with company name and logo listed on the website) . | Promotional Material (company branded products such as booklets, small gifts, etc.) shared with the workshop participants . | Sponsored Lighting Talk at the workshop (e.g., 5-min talk on cool relevant research) . | . Sponsorship Levels . We have Silver, Gold, and Platinum levels. . Silver: Acknowledgment . | Gold: Acknowledgment + Promotional Material . | Platinum: Acknowledgment + Promotional Material + Sponsored Lighting Talk . | . If you are interested in sponsoring the Gaze Meets ML workshop or would like more information, please reach out to gaze.neurips@gmail.com .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/call_for_sponsors/",
          "relUrl": "/call_for_sponsors/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "FAQs",
          "content": "Common FAQs: . Questions related to presentation . Should I upload two separate versions if I have a poster and an oral presentation? No. This is a backup so just an oral presentation is sufficient. However, authors still need to prepare a poster presentation to display in person. | . | Do I still have to upload a pre-recording if I only have a poster? Yes, authors need to upload presentations regardless of their presence (i.e. physical or virtual). Format: 10 min max presentation) | . | How long should the video (and subsequently the oral presentation) be that authors need to upload via the SlidesLive platform? 10 mins max. | . | Poster Dimension instructions: Poster sizes: 24”w x 36”h (61 cm w x 91.5 cm h) | Optional (you do not need to use this service only if you want to): NeurIPS has also arranged for T3Expo to support pre-printing and delivery of posters for you. The deadline for requesting this service is Oct 31. For information and instructions, https://media.neurips.cc/Conferences/NeurIPS2022/NeurIPS_2022_PosterPrinting.pdf | Select: ‘Workshop Posters’. | . | .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/faq/",
          "relUrl": "/faq/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
      ,"page8": {
          "title": "Speakers",
          "content": "We’ve got an amazing lineup of speakers: .   . | . Jürgen Schmidhuber, Ph.D. Director of the AI Initiative at KAUST, Scientific Director, Swiss AI Lab, IDSIA. Since age 15 or so, the main goal of professor Jürgen Schmidhuber has been to build a self-improving Artificial Intelligence (AI) smarter than himself, then retire. His lab’s Deep Learning Neural Networks (NNs) based on ideas published in the “Annus Mirabilis” 1990-1991 have revolutionised machine learning and AI. In 2009, the CTC-trained Long Short-Term Memory (LSTM) of his team was the first recurrent NN to win international pattern recognition competitions. In 2010, his lab’s fast and deep feedforward NNs on GPUs greatly outperformed previous methods, without using any unsupervised pre-training, a popular deep learning strategy that he pioneered in 1991. In 2011, the DanNet of his team was the first feedforward NN to win computer vision contests, achieving superhuman performance. In 2012, they had the first deep NN to win a medical imaging contest (on cancer detection). This deep learning revolution quickly spread from Europe to North America and Asia, and attracted enormous interest from industry. By the mid 2010s, his lab’s NNs were on 3 billion devices, and used billions of times per day through users of the world’s most valuable public companies, e.g., for greatly improved speech recognition on all Android smartphones, greatly improved machine translation through Google Translate and Facebook (over 4 billion LSTM-based translations per day), Apple’s Siri and Quicktype on all iPhones, the answers of Amazon’s Alexa, and numerous other applications. In May 2015, his team published the Highway Net, the first working really deep feedforward NN with hundreds of layers—its open-gated version called ResNet (Dec 2015) has become the most cited NN of the 21st century, LSTM the most cited NN of the 20th (Bloomberg called LSTM the arguably most commercial AI achievement). His lab’s NNs are now heavily used in healthcare and medicine, helping to make human lives longer and healthier. His research group also established the fields of mathematically rigorous universal AI and recursive self-improvement in metalearning machines that learn to learn (since 1987). In 1990, he introduced unsupervised generative adversarial neural networks that fight each other in a minimax game to implement artificial curiosity (the famous GANs are instances thereof). In 1991, he introduced neural fast weight programmers formally equivalent to what’s now called Transformers with linearized self-attention. His formal theory of creativity &amp; curiosity &amp; fun explains art, science, music, and humor. He also generalized algorithmic information theory and the many-worlds theory of physics, and introduced the concept of Low-Complexity Art, the information age’s extreme form of minimal art. He is recipient of numerous awards, author of about 400 peer-reviewed papers, and Chief Scientist of the company NNAISENSE, which aims at building the first practical general purpose AI. He is a frequent keynote speaker, and advising various governments on AI strategies. | .   | . | . Scott W. Linderman, Ph.D. is an Assistant Professor at Stanford University, Statistics Department and the Wu Tsai Neurosciences Institute. His research focuses on machine learning, computational neuroscience, and the general question of how computational and statistical methods can help to decipher neural computation. His work combines novel methodological development in the areas of state space models, deep generative models, point processes, and approximate Bayesian inference with applied statistical analyses of large-scale neural and behavioral data. Previously, he was a postdoctoral fellow with David Blei and Liam Paninski at Columbia University and a graduate student at Harvard with Ryan Adams. His work has been recognized with a Savage Award from the International Society for Bayesian Analysis, an AISTATS Best Paper Award, and a Sloan Fellowship. | .   | . | . Gabriel A. Silva, Ph.D. is a Professor in the Department of Bioengineering and the Department of Neurosciences at the University of California San Diego. He holds the J. Robert Beyster Endowed Chair in Engineering, is the Founding Director of the Center for Engineered Natural Intelligence, and Associate Director of the Kavli Institute for Brain and Mind. He is an affiliated faculty member in the Department of NanoEngineering, and a faculty member in the BioCircuits Institute, the Neurosciences Graduate Program, Computational Neurobiology Program, and Institute for Neural Computation. Silva’s work includes experimental neuroscience, theoretical and computational neuroscience, and neural engineering. He has worked on mathematical and physical modeling and simulations of neural processes at molecular, cellular, and systems scales, for the purpose of understanding how structures in the brain represent and process information, with a particular focus on modeling the calcium signaling dynamics of astrocyte neural glial cells. Prof. Silva has received numerous awards and recognitions for his research and teaching, including the IEEE/EMBS Excellence in Neural Engineering award, Wallace Coulter Foundation Early Career award, Faculty of the Year award for undergraduate education from the Tau Beta Pi Engineering Honors Society, selection to “Nanoscience: The best of NATURE publications”, and the YC Fung Young Investigator Award and Medal. He holds a Ph.D. in bioengineering and neurophysiology from the University of Illinois at Chicago, followed by a postdoctoral fellowship in the Institute for BioNanotechnology and Medicine (IBNAM) and the Department of Neurology at Northwestern University. | .   | . | . Claudia Mello-Thoms, MS, Ph.D. is a Research Associate Professor of Radiology and Biomedical Engineering at the University of Iowa, U.S. Her research interests are visual search, image perception, image understanding, human decision making and integration of artificial intelligence with human processes. She has shown that breast radiologists have individualized error making patterns (False Negatives) that they repeat time and again, and her research has modeled these decision-making behaviors in the reading of digital mammograms using eye position recording and convolutional neural networks. She has also modeled breast radiologists and radiology residents’ overall decision-making patterns (including True and False Positives, and False Negatives) using eye gaze and machine learning. This research has shown that on average 82% of the decisions made by the breast radiologists and 67% of the decisions made by the residents could be predicted by the machine learning model. | .   | . | . Miguel P. Eckstein, Ph.D. is the Mellichamp Professor in Mind and Machine Intelligence, Department of Psychological and Brain Sciences, and affiliated faculty in Electrical and Computer Engineering and Computer at the University of California, Santa Barbara. His research aims at uncovering the computational and neuroscience bases of human visual search with applications to medical image perception. Related to medical imaging, he worked on one of the first applications of computational model observers to real clinical images to optimize medical image quality. In recent years, he has worked on understanding the differences in how CNN-based object detectors and humans search in real-world and medical images and developing eye movement models of visual search that incorporate computer vision architectures. He has shown how radiologist performance when searching 3D images is severely limited by under-exploration of the data and the bottlenecks of visual processing away from the point of fixation (visual periphery). His work has been recognized with a National Science Foundation CAREER award, a National Academy Sciences Troland award, and a Guggenheim fellowship. | .   | . | . Tobias Gerstenberg, MSc, Ph.D. is an Assistant Professor of Psychology at Stanford University. He leads the Causality in Cognition Lab (CiCL). CiCL studies the role of causality in people’s understanding of the world, and of each other. Professor Gerstenberg’s research is highly interdisciplinary. It combines ideas from philosophy, linguistics, computer science, and the legal sciences to better understand higher-level cognitive phenomena such as causal inference and moral judgment. The CiCL’s research uses a variety of methods that include computational modeling, online experiments, eye-tracking experiments, as well as developmental studies with children. Professor Gerstenberg’s work has appeared in top journals including Psychological Review, Journal of Experimental Psychology: General, Psychological Science, Cognitive Psychology, Cognition, and Cognitive Science. | .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/",
          "relUrl": "/speakers/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "Sponsors",
          "content": "We are excited to have the following organizations sponsor our workshop: . . . . . .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/sponsors/",
          "relUrl": "/sponsors/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gaze-meets-ml.github.io/gaze_ml_2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}