<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Blog | NeurIPS 2022 Gaze Meets ML Workshop</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Blog" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bridging human and machine attention" />
<meta property="og:description" content="Bridging human and machine attention" />
<link rel="canonical" href="https://gaze-meets-ml.github.io/gaze_ml_2022/blog/" />
<meta property="og:url" content="https://gaze-meets-ml.github.io/gaze_ml_2022/blog/" />
<meta property="og:site_name" content="NeurIPS 2022 Gaze Meets ML Workshop" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Blog" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Bridging human and machine attention","headline":"Blog","url":"https://gaze-meets-ml.github.io/gaze_ml_2022/blog/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/gaze_ml_2022/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gaze-meets-ml.github.io/gaze_ml_2022/feed.xml" title="NeurIPS 2022 Gaze Meets ML Workshop" /><link rel="shortcut icon" type="image/x-icon" href="/gaze_ml_2022/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/gaze_ml_2022/">NeurIPS 2022 Gaze Meets ML Workshop</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gaze_ml_2022/presentations/">Presentations</a><a class="page-link" href="/gaze_ml_2022/blog/">Blog</a><a class="page-link" href="/gaze_ml_2022/faq/">FAQs</a><a class="page-link" href="/gaze_ml_2022/speakers/">Speakers</a><a class="page-link" href="/gaze_ml_2022/sponsors/">Sponsors</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">Blog</h1><!-- > 12.28.2022 - Gaze meets ML workshop organizing committee blog post -->

<p>We are very delighted to share with the research community that the first <span style="color:MediumSeaGreen">Gaze Meets ML</span> workshop successfully took place at <a href="https://nips.cc/Conferences/2022/ScheduleMultitrack?event=49990">NeurIPS</a> in New Orleans on December 3rd, 2022. This one-day event was attended by more than 50 participants.</p>

<p>We had the honor to host <a href="https://people.idsia.ch/~juergen/">Prof. Jürgen Schmidhuber</a> as a remote keynote speaker inaugurating the workshop and offering a history of the efforts on visual attention in machine learning. Prof. Schmidhuber started his talk with the attentive neural networks proposed in the 90s that mimic foveas and learn internal spotlights of attention in a fashion of early Transformer-like systems. He then continued the discussion by offering insights on the research related to representing percepts and action plans in hierarchical neural networks, at multiple levels of abstraction, and multiple time scales, which are key mechanisms of complex activities such as visual attention.</p>

<p align="center" width="100%">
    <img width="100%" src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/platinum.png" /> 
</p>

<p><a href="https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/KeynoteJürgenSchmidhuber.jpg"><img src="https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/KeynoteJürgenSchmidhuber.jpg" /></a></p>

<!-- ![keynote](https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/KeynoteJürgenSchmidhuber.jpg) -->

<p>During the day we had the opportunity to have <a href="https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/">speakers</a> present visual attention research from diverse backgrounds. Specifically, Associate Prof. Claudia Mello-Thoms from the University of Iowa presented work on eye tracking for radiology through a large study. Prof. Miguel P. Eckstein from the University of California, Santa Barbara offered insights on the interpretation of volumetric radiology images using eye tracking. Assistant Prof. Tobias Gerstenberg from Stanford University provided studies demonstrating how eye-tracking allows peek into the inner workings of the human mind. Finally, Assistant Prof. Scott W. Linderman from Stanford University presented neuroscience introduced participants to his research on relationship between face, eye movements and visual coding and how new ML methods for relating these neural and behavioral time series are needed to be developed.</p>

<p><a href="https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/speakers.jpg"><img src="https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/speakers.jpg" /></a></p>

<!-- ![speakers](https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/speakers.jpg) -->

<p>Overall, we had 7 oral presentations and 11 poster presentations covering various research areas of visual attention from utilizing gaze in federated settings to benchmark datasets that assess the locus of a participant’s overt visual attention. The audience size allowed us to host a 1-hour breakout session. This offered a unique opportunity to have small group discussions on a variety of topics raised by the participants. Topics included <em>“Fairness in gaze estimation”</em>, <em>“Understanding peripheral visual attention”</em>, and <em>“Annotating through eye gaze: from explicit to implicit annotation”</em>. This direct involvement was a centerpiece to the nature of the workshop by enabling interaction and idea exchange between multi-disciplinary experts from the workshop audience.</p>

<p>Thanks to the generous support of our main sponsor, <a href="https://www.gazept.com/">Gazepoint</a>, we were able to provide 2 high-end <a href="https://www.gazept.com/product/analysis-ultimate-bundle/">GP3 Eye Trackers </a> to the Best Paper Award winners! These eye trackers came along with the Gazepoint Analysis UX Edition software that allows researchers to collect multimodal data out-of-the-box including eye tracking, voice, and face recording.<br />
<img src="https://github.com/gaze-meets-ml/gaze_ml_2022/blob/master/images/blog/winners_gazepoint.jpg" alt="gp3" /></p>

<p>The Gaze Meets ML workshop is focused on visual attention, an active multi-disciplinary area of research. With its first version, we believe that we managed to bring together diverse communities and build bridges of communication and understanding. Because of the strong interest and the potential for transformative research in visual attention and gaze-assisted machine learning, we are excited at the prospect of continuing this effort, hoping to strengthen this community and build the foundation for interdisciplinary impactful collaborations.</p>



  

  <!-- Hide posts if front matter flag hide:true -->
  
  

  <!-- Sort posts by rank, then date -->
  
  
  

 
  

   <!-- Assemble final sorted posts array -->
  
  </div>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/gaze_ml_2022/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://gaze-meets-ml.github.io/gaze_ml_2022/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Bridging human and machine attention</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://twitter.com/Gaze_Meets_ML" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
