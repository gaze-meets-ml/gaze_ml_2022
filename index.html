<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>NeurIPS 2022 Gaze Meets ML Workshop | Bridging human and machine attention</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="NeurIPS 2022 Gaze Meets ML Workshop" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bridging human and machine attention" />
<meta property="og:description" content="Bridging human and machine attention" />
<link rel="canonical" href="https://gaze-meets-ml.github.io/gaze_ml_2022/" />
<meta property="og:url" content="https://gaze-meets-ml.github.io/gaze_ml_2022/" />
<meta property="og:site_name" content="NeurIPS 2022 Gaze Meets ML Workshop" />
<meta property="og:image" content="https://gaze-meets-ml.github.io/gaze_ml_2022/images/eye-gaze-logo2.png" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://gaze-meets-ml.github.io/gaze_ml_2022/images/eye-gaze-logo2.png" />
<meta property="twitter:title" content="NeurIPS 2022 Gaze Meets ML Workshop" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Bridging human and machine attention","headline":"NeurIPS 2022 Gaze Meets ML Workshop","image":"https://gaze-meets-ml.github.io/gaze_ml_2022/images/eye-gaze-logo2.png","name":"NeurIPS 2022 Gaze Meets ML Workshop","url":"https://gaze-meets-ml.github.io/gaze_ml_2022/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/gaze_ml_2022/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gaze-meets-ml.github.io/gaze_ml_2022/feed.xml" title="NeurIPS 2022 Gaze Meets ML Workshop" /><link rel="shortcut icon" type="image/x-icon" href="/gaze_ml_2022/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/gaze_ml_2022/">NeurIPS 2022 Gaze Meets ML Workshop</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gaze_ml_2022/call_for_sponsors/">Call For Sponsors</a><a class="page-link" href="/gaze_ml_2022/speakers/">Speakers</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><p>We are excited to host the first-ever <span style="color:MediumSeaGreen">Gaze Meets ML</span> workshop on <b>December 3rd, 2022</b> in conjunction with NeurIPS 2022. We’ve got a great lineup of <a href="https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/">speakers</a>. <b>If you are interested in sponsoring, please find more information <a href="https://gaze-meets-ml.github.io/gaze_ml_2022/call_for_sponsors/">here</a></b>.</p>

<p><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/eye-gaze-logo2.png" width="3000" /></p>

<h1 id="about">About</h1>
<p>Eye gaze has proven to be a cost-efficient way to collect large-scale physiological data that can reveal the underlying
human attentional patterns in real life workflows, and thus has long been explored as a signal to directly measure
human-related cognition in various domains. Physiological data (including but not limited to eye gaze) offer new
perception capabilities, which could be used in several ML domains, e.g., egocentric perception, embodiedAI, NLP, etc.
They can help infer human perception, intentions, beliefs, goals and other cognition properties that are much needed for
human-AI interactions and agent coordination. In addition, large collections of eye-tracking data have enabled
data-driven modeling of human visual attention mechanisms, both for saliency or scanpath prediction, with twofold
advantages: from the neuroscientific perspective to understand biological mechanisms better, from the AI perspective to
equip agents with the ability to mimic or predict human behavior and improve interpretability and interactions.</p>

<p>With the emergence of immersive technologies, now more than any time there is a need for experts of various backgrounds
(e.g., machine learning, vision, and neuroscience communities) to share expertise and contribute to a deeper
understanding of the intricacies of cost-efficient human supervision signals (e.g., eye-gaze) and their utilization
towards by bridging human cognition and AI in machine learning research and development. The goal of this workshop is to
bring together an active research community to collectively drive progress in defining and addressing core problems in
gaze-assisted machine learning.</p>

<h1 id="call-for-papers">Call for Papers</h1>
<p>We welcome submissions that present aspects of eye-gaze in regards to cognitive science, psychophysiology and computer
science, or propose methods on integrating eye gaze into machine learning. We are also looking for applications from radiology, AR/VR,
autonomous driving, etc. that introduce methods and models utilizing eye gaze technology in their respective domains.
Topics of interest include but are not limited to the following:</p>
<ul>
    <li>Understanding the neuroscience of eye-gaze and perception.</li>
    <li>State of the art in incorporating machine learning and eye-tracking.</li>
    <li>Annotation and ML supervision with eye-gaze.</li>
    <li>Attention mechanisms and their correlation with eye-gaze.</li>
    <li>Methods for gaze estimation and prediction using machine learning.</li>
    <li>Unsupervised ML using eye gaze information for feature importance/selection.</li>
    <li>Understanding human intention and goal inference.</li>
    <li>Using saccadic vision for ML applications.</li>
    <li>Use of gaze for human-AI interaction and agent coordination in multi-agent environments.</li>
    <li>Eye gaze used for AI, e.g., NLP, Computer Vision, RL, Explainable AI, Embodied AI, Trustworthy AI.</li>
    <li>Gaze applications in cognitive psychology, radiology, neuroscience, AR/VR, autonomous cars, privacy, etc.</li>
</ul>

<h1 id="important-dates">Important Dates</h1>
<ul>Submission due: <b>1st October 2022</b></ul>
<ul>Reviewing starts: <b>4th October 2022</b></ul>
<ul>Reviewing ends: <b>16th October 2022</b></ul>
<ul>Notification of acceptance: <b>20th October 2022</b> </ul>
<!-- <ul>Video upload: <b>1st November 2022</b></ul> -->
<ul>Camera ready: <b>5th November 2022</b></ul>
<p>All dates listed are 23:59 <a href="https://time.is/Anywhere_on_Earth">Anywhere on Earth</a></p>

<h1 id="submissions">Submissions</h1>
<ul>
    <li><a href="https://openreview.net/group?id=NeurIPS.cc/2022/Workshop/GMML">Open Review Submission Portal</a></li>
    <li> Submission format: anonymously submit full papers (8 or 9 pages) using:
        <ul>
            <li><a href="https://media.neurips.cc/Conferences/NeurIPS2022/Styles/neurips_2022.tex">neurips_2022.tex --
                    LaTeX
                    template</a></li>
            <li><a href="https://media.neurips.cc/Conferences/NeurIPS2022/Styles/neurips_2022.sty">neurips_2022.sty --
                    style
                    file
                    for LaTeX 2e</a></li>
            <li><a href="https://media.neurips.cc/Conferences/NeurIPS2022/Styles/neurips_2022.pdf">neurips_2022.pdf --
                    example
                    PDF
                    output generated by running “pdflatex”</a></li>
        </ul>
    </li>
</ul>

<ul>
    <li>References and appendix should be appended into the same (single) PDF document, and do not count towards the
        page count.
    </li>
</ul>

<h1 id="organizers">Organizers</h1>
<ul>
<li> Ismini Lourentzou (Virginia Tech) </li>
<li> Joy Tzung-yu Wu (Stanford, IBM Research) </li>
<li> Satyananda Kashyap (IBM Research)</li>
<li> Alexandros Karargyris (IHU Strasbourg)</li>
<li> Leo Antony Celi (MIT)</li>
<li> Ban Kawas (Meta, Reality Labs Research)</li>
<li> Sachin Talathi (Meta, Reality Labs Research)</li>
 </ul>




  

  <!-- Hide posts if front matter flag hide:true -->
  
  

  <!-- Sort posts by rank, then date -->
  
  
  

 
  

   <!-- Assemble final sorted posts array -->
  
  </div>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/gaze_ml_2022/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/gaze_ml_2022/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Bridging human and machine attention</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://twitter.com/Gaze_Meets_ML" target="_blank" title="Gaze_Meets_ML"><svg class="svg-icon grey"><use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
