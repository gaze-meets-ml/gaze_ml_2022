<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Speakers | NeurIPS 2022 Gaze Meets ML Workshop</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Speakers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bridging human and machine attention" />
<meta property="og:description" content="Bridging human and machine attention" />
<link rel="canonical" href="https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/" />
<meta property="og:url" content="https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/" />
<meta property="og:site_name" content="NeurIPS 2022 Gaze Meets ML Workshop" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Speakers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Bridging human and machine attention","headline":"Speakers","url":"https://gaze-meets-ml.github.io/gaze_ml_2022/speakers/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/gaze_ml_2022/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gaze-meets-ml.github.io/gaze_ml_2022/feed.xml" title="NeurIPS 2022 Gaze Meets ML Workshop" /><link rel="shortcut icon" type="image/x-icon" href="/gaze_ml_2022/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/gaze_ml_2022/">NeurIPS 2022 Gaze Meets ML Workshop</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/gaze_ml_2022/call_for_sponsors/">Call For Sponsors</a><a class="page-link" href="/gaze_ml_2022/faq/">FAQs</a><a class="page-link" href="/gaze_ml_2022/speakers/">Speakers</a><a class="page-link" href="/gaze_ml_2022/sponsors/">Sponsors</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading">Speakers</h1><p>We’ve got an amazing lineup of speakers:</p>

<table>
  <thead>
    <tr>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Juergen-Schmidhuber.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="schmidhuber"></a><a href="https://people.idsia.ch/~juergen/"><b>Jürgen Schmidhuber</b></a>, Ph.D. Director of the AI Initiative at KAUST, Scientific Director, Swiss AI Lab, IDSIA. Since age 15 or so, the main goal of professor Jürgen Schmidhuber has been to build a self-improving Artificial Intelligence (AI) smarter than himself, then retire. His lab’s Deep Learning Neural Networks (NNs) based on ideas published in the “Annus Mirabilis” 1990-1991 have revolutionised machine learning and AI. In 2009, the CTC-trained Long Short-Term Memory (LSTM) of his team was the first recurrent NN to win international pattern recognition competitions. In 2010, his lab’s fast and deep feedforward NNs on GPUs greatly outperformed previous methods, without using any unsupervised pre-training, a popular deep learning strategy that he pioneered in 1991. In 2011, the DanNet of his team was the first feedforward NN to win computer vision contests, achieving superhuman performance. In 2012, they had the first deep NN to win a medical imaging contest (on cancer detection). This deep learning revolution quickly spread from Europe to North America and Asia, and attracted enormous interest from industry. By the mid 2010s, his lab’s NNs were on 3 billion devices, and used billions of times per day through users of the world’s most valuable public companies, e.g., for greatly improved speech recognition on all Android smartphones, greatly improved machine translation through Google Translate and Facebook (over 4 billion LSTM-based translations per day), Apple’s Siri and Quicktype on all iPhones, the answers of Amazon’s Alexa, and numerous other applications. In May 2015, his team published the Highway Net, the first working really deep feedforward NN with hundreds of layers—its open-gated version called ResNet (Dec 2015) has become the most cited NN of the 21st century, LSTM the most cited NN of the 20th (Bloomberg called LSTM the arguably most commercial AI achievement). His lab’s NNs are now heavily used in healthcare and medicine, helping to make human lives longer and healthier. His research group also established the fields of mathematically rigorous universal AI and recursive self-improvement in metalearning machines that learn to learn (since 1987). In 1990, he introduced unsupervised generative adversarial neural networks that fight each other in a minimax game to implement artificial curiosity (the famous GANs are instances thereof). In 1991, he introduced neural fast weight programmers formally equivalent to what’s now called Transformers with linearized self-attention. His formal theory of creativity &amp; curiosity &amp; fun explains art, science, music, and humor. He also generalized algorithmic information theory and the many-worlds theory of physics, and introduced the concept of Low-Complexity Art, the information age’s extreme form of minimal art. He is recipient of numerous awards, author of about 400 peer-reviewed papers, and Chief Scientist of the company NNAISENSE, which aims at building the first practical general purpose AI. He is a frequent keynote speaker, and advising various governments on AI strategies.</td>
    </tr>
    <tr>
      <td> </td>
    </tr>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Scott_Linderman_cropped.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="linderman"></a><a href="https://web.stanford.edu/~swl1/"><b>Scott W. Linderman</b></a>, Ph.D. is an Assistant Professor at Stanford University, Statistics Department and the Wu Tsai Neurosciences Institute. His research focuses on machine learning, computational neuroscience, and the general question of how computational and statistical methods can help to decipher neural computation. His work combines novel methodological development in the areas of state space models, deep generative models, point processes, and approximate Bayesian inference with applied statistical analyses of large-scale neural and behavioral data. Previously, he was a postdoctoral fellow with David Blei and Liam Paninski at Columbia University and a graduate student at Harvard with Ryan Adams. His work has been recognized with a Savage Award from the International Society for Bayesian Analysis, an AISTATS Best Paper Award, and a Sloan Fellowship.</td>
    </tr>
    <tr>
      <td> </td>
    </tr>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Gabriel_Silva_cropped.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="silva"></a><a href="http://www.silva.ucsd.edu/silvabio"><b> Gabriel A. Silva</b></a>, Ph.D. is a Professor in the Department of Bioengineering and the Department of Neurosciences at the University of California San Diego. He holds the J. Robert Beyster Endowed Chair in Engineering, is the Founding Director of the Center for Engineered Natural Intelligence, and Associate Director of the Kavli Institute for Brain and Mind. He is an affiliated faculty member in the Department of NanoEngineering, and a faculty member in the BioCircuits Institute, the Neurosciences Graduate Program, Computational Neurobiology Program, and Institute for Neural Computation. Silva’s work includes experimental neuroscience, theoretical and computational neuroscience, and neural engineering. He has worked on mathematical and physical modeling and simulations of neural processes at molecular, cellular, and systems scales, for the purpose of understanding how structures in the brain represent and process information, with a particular focus on modeling the calcium signaling dynamics of astrocyte neural glial cells. Prof. Silva has received numerous awards and recognitions for his research and teaching, including the IEEE/EMBS Excellence in Neural Engineering award, Wallace Coulter Foundation Early Career award, Faculty of the Year award for undergraduate education from the Tau Beta Pi Engineering Honors Society, selection to “Nanoscience: The best of NATURE publications”, and the YC Fung Young Investigator Award and Medal. He holds a Ph.D. in bioengineering and neurophysiology from the University of Illinois at Chicago, followed by a postdoctoral fellowship in the Institute for BioNanotechnology and Medicine (IBNAM) and the Department of Neurology at Northwestern University.</td>
    </tr>
    <tr>
      <td> </td>
    </tr>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Claudia-Mello-Thoms_cropped.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="mellothoms"></a><a href="https://medicine.uiowa.edu/radiology/profile/claudia-mello-thoms"><b>Claudia Mello-Thoms</b></a>, MS, Ph.D. is a Research Associate Professor of Radiology and Biomedical Engineering at the University of Iowa, U.S. Her research interests are visual search, image perception, image understanding, human decision making and integration of artificial intelligence with human processes. She has shown that breast radiologists have individualized error making patterns (False Negatives) that they repeat time and again, and her research has modeled these decision-making behaviors in the reading of digital mammograms using eye position recording and convolutional neural networks. She has also modeled breast radiologists and radiology residents’ overall decision-making patterns (including True and False Positives, and False Negatives) using eye gaze and machine learning. This research has shown that on average 82% of the decisions made by the breast radiologists and 67% of the decisions made by the residents could be predicted by the machine learning model.</td>
    </tr>
    <tr>
      <td> </td>
    </tr>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Eckstein_Miguel_cropped.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="eckstein"></a><a href="https://psych.ucsb.edu/people/faculty/miguel-eckstein"><b>Miguel P. Eckstein</b></a>, Ph.D. is the Mellichamp Professor in Mind and Machine Intelligence, Department of Psychological and Brain Sciences, and affiliated faculty in Electrical and Computer Engineering and Computer at the University of California, Santa Barbara.  His research aims at uncovering the computational and neuroscience bases of human visual search with applications to medical image perception.  Related to medical imaging, he worked on one of the first applications of computational model observers to real clinical images to optimize medical image quality. In recent years, he has worked on understanding the differences in how CNN-based object detectors and humans search in real-world and medical images and developing eye movement models of visual search that incorporate computer vision architectures. He has shown how radiologist performance when searching 3D images is severely limited by under-exploration of the data and the bottlenecks of visual processing away from the point of fixation (visual periphery).  His work has been recognized with a National Science Foundation CAREER award, a National Academy Sciences Troland award, and a Guggenheim fellowship.</td>
    </tr>
    <tr>
      <td> </td>
    </tr>
    <tr>
      <td><img src="https://gaze-meets-ml.github.io/gaze_ml_2022/images/Tobias_Gerstenberg_cropped.jpg" width="200" /></td>
    </tr>
    <tr>
      <td><a name="gerstenberg"></a><a href="https://cicl.stanford.edu/member/tobias_gerstenberg/"><b>Tobias Gerstenberg</b></a>, MSc, Ph.D. is an Assistant Professor of Psychology at Stanford University. He leads the <a href="http://cicl.stanford.edu">Causality in Cognition Lab (CiCL)</a>. CiCL studies the role of causality in people’s understanding of the world, and of each other. Professor Gerstenberg’s research is highly interdisciplinary. It combines ideas from philosophy, linguistics, computer science, and the legal sciences to better understand higher-level cognitive phenomena such as causal inference and moral judgment. The CiCL’s research uses a variety of methods that include computational modeling, online experiments, eye-tracking experiments, as well as developmental studies with children. Professor Gerstenberg’s work has appeared in top journals including Psychological Review, Journal of Experimental Psychology: General, Psychological Science, Cognitive Psychology, Cognition, and Cognitive Science.</td>
    </tr>
  </tbody>
</table>



  

  <!-- Hide posts if front matter flag hide:true -->
  
  

  <!-- Sort posts by rank, then date -->
  
  
  

 
  

   <!-- Assemble final sorted posts array -->
  
  </div>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/gaze_ml_2022/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/gaze_ml_2022/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Bridging human and machine attention</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://twitter.com/Gaze_Meets_ML" target="_blank" title="Gaze_Meets_ML"><svg class="svg-icon grey"><use xlink:href="/gaze_ml_2022/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
